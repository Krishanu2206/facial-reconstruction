{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T07:56:26.944818Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/raw/cctv_footage/surveillance_cameras_all\"\n",
    "low_res_img_paths = os.listdir(data_path)\n",
    "\n",
    "idx = 3\n",
    "img = Image.open(data_path + f\"/{low_res_img_paths[idx]}\")\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.Resize((286, 286)),\n",
    "    transforms.RandomRotation((-15, 15)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomPerspective(0.5),\n",
    "    transforms.RandomCrop((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transformation(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T07:56:21.551115Z",
     "start_time": "2024-09-18T07:56:21.365348Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LR = 0.0003\n",
    "CHANNELS = 3\n",
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T07:56:07.597046Z",
     "start_time": "2024-09-18T07:56:07.374436Z"
    }
   },
   "outputs": [],
   "source": [
    "lowResImagesPath = \"data/raw/cctv_footage/surveillance_cameras_all\"\n",
    "lowResImages = os.listdir(lowResImagesPath) # 2860 \n",
    "\n",
    "highResImagesPath = \"data/raw/high_quality_images/mugshot_frontal_original_all\"\n",
    "highResImages = os.listdir(highResImagesPath)  # 130\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.Resize((286, 286)),\n",
    "    transforms.RandomRotation((-15, 15)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomPerspective(0.5),\n",
    "    transforms.RandomCrop((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, lowResImagesPath, highResImagesPath, transform=None, target_transform=None):\n",
    "        self.lowResImagesPath = lowResImagesPath\n",
    "        self.highResImagesPath = highResImagesPath\n",
    "        self.lowResImages = os.listdir(lowResImagesPath)\n",
    "        self.highResImages = os.listdir(highResImagesPath)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lowResImages)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        _low_image_name = self.lowResImages[idx]\n",
    "        _low_res_image_path = os.path.join(self.lowResImagesPath, _low_image_name)\n",
    "        _subject = _low_image_name.split(\"_\")[0]\n",
    "        _high_image_name = _subject + \"_frontal.jpg\"\n",
    "        _high_image_path = os.path.join(self.highResImagesPath, _high_image_name)\n",
    "        \n",
    "        _low_res_img = Image.open(_low_res_image_path)\n",
    "        _high_res_img = Image.open(_high_image_path)\n",
    "        if self.transform:\n",
    "            _low_res_img = self.transform(_low_res_img)\n",
    "        if self.target_transform:\n",
    "            _high_res_img = self.target_transform(_high_res_img)\n",
    "        \n",
    "        return _low_res_img, _high_res_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:45:33.507614Z",
     "start_time": "2024-09-17T19:45:33.489575Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(lowResImagesPath=\"data/processed/train/low_res\",\n",
    "                              highResImagesPath=\"data/processed/train/high_res\",\n",
    "                              transform=transformation,\n",
    "                              target_transform=target_transform)\n",
    "\n",
    "test_dataset = CreateDataset(lowResImagesPath=\"data/processed/test/low_res\",\n",
    "                             highResImagesPath=\"data/processed/test/high_res\",\n",
    "                             transform=transformation,\n",
    "                             target_transform=target_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:46:11.682018Z",
     "start_time": "2024-09-17T19:46:08.886031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:53:18.686565Z",
     "start_time": "2024-09-17T19:53:18.676390Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnn_block(in_channels,out_channels,kernel_size,stride=1,padding=0, first_layer = False):\n",
    "\n",
    "   if first_layer:\n",
    "       return nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding)\n",
    "   else:\n",
    "       return nn.Sequential(\n",
    "           nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding),\n",
    "           nn.BatchNorm2d(out_channels,momentum=0.1,eps=1e-5),\n",
    "           )\n",
    "\n",
    "def tcnn_block(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0, first_layer = False):\n",
    "   if first_layer:\n",
    "       return nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,output_padding=output_padding)\n",
    "\n",
    "   else:\n",
    "       return nn.Sequential(\n",
    "           nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,output_padding=output_padding),\n",
    "           nn.BatchNorm2d(out_channels,momentum=0.1,eps=1e-5),\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:54:16.300940Z",
     "start_time": "2024-09-17T19:54:16.286427Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    " def __init__(self,instance_norm=False):#input : 256x256\n",
    "   super(Generator,self).__init__()\n",
    "   self.e1 = cnn_block(c_dim,gf_dim,4,2,1, first_layer = True)\n",
    "   self.e2 = cnn_block(gf_dim,gf_dim*2,4,2,1,)\n",
    "   self.e3 = cnn_block(gf_dim*2,gf_dim*4,4,2,1,)\n",
    "   self.e4 = cnn_block(gf_dim*4,gf_dim*8,4,2,1,)\n",
    "   self.e5 = cnn_block(gf_dim*8,gf_dim*8,4,2,1,)\n",
    "   self.e6 = cnn_block(gf_dim*8,gf_dim*8,4,2,1,)\n",
    "   self.e7 = cnn_block(gf_dim*8,gf_dim*8,4,2,1,)\n",
    "   self.e8 = cnn_block(gf_dim*8,gf_dim*8,4,2,1, first_layer=True)\n",
    "\n",
    "   self.d1 = tcnn_block(gf_dim*8,gf_dim*8,4,2,1)\n",
    "   self.d2 = tcnn_block(gf_dim*8*2,gf_dim*8,4,2,1)\n",
    "   self.d3 = tcnn_block(gf_dim*8*2,gf_dim*8,4,2,1)\n",
    "   self.d4 = tcnn_block(gf_dim*8*2,gf_dim*8,4,2,1)\n",
    "   self.d5 = tcnn_block(gf_dim*8*2,gf_dim*4,4,2,1)\n",
    "   self.d6 = tcnn_block(gf_dim*4*2,gf_dim*2,4,2,1)\n",
    "   self.d7 = tcnn_block(gf_dim*2*2,gf_dim*1,4,2,1)\n",
    "   self.d8 = tcnn_block(gf_dim*1*2,c_dim,4,2,1, first_layer = True)#256x256\n",
    "   self.tanh = nn.Tanh()\n",
    "\n",
    " def forward(self,x):\n",
    "   e1 = self.e1(x)\n",
    "   e2 = self.e2(nn.LeakyReLU(0.2)(e1))\n",
    "   e3 = self.e3(nn.LeakyReLU(0.2)(e2))\n",
    "   e4 = self.e4(nn.LeakyReLU(0.2)(e3))\n",
    "   e5 = self.e5(nn.LeakyReLU(0.2)(e4))\n",
    "   e6 = self.e6(nn.LeakyReLU(0.2)(e5))\n",
    "   e7 = self.e7(nn.LeakyReLU(0.2)(e6))\n",
    "   e8 = self.e8(nn.LeakyReLU(0.2)(e7))\n",
    "   d1 = torch.cat([nn.Dropout(0.5)(self.d1(nn.ReLU()(e8))),e7],1)\n",
    "   d2 = torch.cat([nn.Dropout(0.5)(self.d2(nn.ReLU()(d1))),e6],1)\n",
    "   d3 = torch.cat([nn.Dropout(0.5)(self.d3(nn.ReLU()(d2))),e5],1)\n",
    "   d4 = torch.cat([self.d4(nn.ReLU()(d3)),e4],1)\n",
    "   d5 = torch.cat([self.d5(nn.ReLU()(d4)),e3],1)\n",
    "   d6 = torch.cat([self.d6(nn.ReLU()(d5)),e2],1)\n",
    "   d7 = torch.cat([self.d7(nn.ReLU()(d6)),e1],1)\n",
    "   d8 = self.d8(nn.ReLU()(d7))\n",
    "\n",
    "   return self.tanh(d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:54:31.601258Z",
     "start_time": "2024-09-17T19:54:31.590732Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    " def __init__(self,instance_norm=False):#input : 256x256\n",
    "   super(Discriminator,self).__init__()\n",
    "   self.conv1 = cnn_block(c_dim*2,df_dim,4,2,1, first_layer=True) # 128x128\n",
    "   self.conv2 = cnn_block(df_dim,df_dim*2,4,2,1)# 64x64\n",
    "   self.conv3 = cnn_block(df_dim*2,df_dim*4,4,2,1)# 32 x 32\n",
    "   self.conv4 = cnn_block(df_dim*4,df_dim*8,4,1,1)# 31 x 31\n",
    "   self.conv5 = cnn_block(df_dim*8,1,4,1,1, first_layer=True)# 30 x 30\n",
    "\n",
    "   self.sigmoid = nn.Sigmoid()\n",
    " def forward(self, x, y):\n",
    "   O = torch.cat([x,y],dim=1)\n",
    "   O = nn.LeakyReLU(0.2)(self.conv1(O))\n",
    "   O = nn.LeakyReLU(0.2)(self.conv2(O))\n",
    "   O = nn.LeakyReLU(0.2)(self.conv3(O))\n",
    "   O = nn.LeakyReLU(0.2)(self.conv4(O))\n",
    "   O = self.conv5(O)\n",
    "\n",
    "   return self.sigmoid(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T19:54:35.034250Z",
     "start_time": "2024-09-17T19:54:35.026893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 4\n",
    "workers = 2\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "gf_dim = 64\n",
    "df_dim = 64\n",
    "\n",
    "L1_lambda = 100.0\n",
    "\n",
    "in_w = in_h = 256\n",
    "c_dim = 3\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32, 3, 256, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low, high = next(iter(train_loader))\n",
    "low.shape, high.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Generator()\n",
    "op = g_model(low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_n = op[0].squeeze().detach().permute(1, 2, 0).type(torch.float).numpy() * 256\n",
    "\n",
    "op_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.imshow(op_n)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g = Generator().to(DEVICE)\n",
    "model_d = Discriminator().to(DEVICE)\n",
    "\n",
    "optimizer_g = torch.optim.Adam(model_g.parameters(), lr=2e-4,betas=(0.5,0.999))\n",
    "optimizer_d = torch.optim.Adam(model_d.parameters(), lr=2e-5,betas=(0.5,0.999))\n",
    "\n",
    "bce_criterion = nn.BCELoss()\n",
    "L1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for id, (low_res, high_res) in enumerate(train_loader):\n",
    "        real_images = high_res.to(DEVICE)\n",
    "\n",
    "        # train the discriminator\n",
    "        b_size = low_res.size(0)\n",
    "\n",
    "        real_labels = torch.ones((b_size, 1, 30, 30)).to(DEVICE)\n",
    "        fake_labels = torch.zeros((b_size, 1, 30, 30)).to(DEVICE)\n",
    "\n",
    "        fake_images = model_g(low_res.to(DEVICE))\n",
    "        real_patch = model_d(low_res.to(DEVICE), real_images)\n",
    "\n",
    "        fake_patch = model_d(low_res.to(DEVICE), fake_images.detach())\n",
    "\n",
    "        d_loss_real = bce_criterion(real_patch, real_labels)\n",
    "        d_loss_fake = bce_criterion(fake_patch, fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # train the generator\n",
    "        fake_images = model_g(low_res.to(DEVICE))  # Generate new fake images\n",
    "        fake_patch = model_d(low_res.to(DEVICE), fake_images)\n",
    "        \n",
    "        fake_gan_loss = bce_criterion(fake_patch, real_labels)\n",
    "        L1_loss = L1_criterion(fake_images, real_images)\n",
    "        g_loss = fake_gan_loss + L1_lambda * L1_loss\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{id+1}/{len(train_loader)}], G_loss: {g_loss.item():.4f}, D_loss: {d_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
